------------------------------------------------------------------------------------
init request: 
```
user-input/prompt: "@data analyze data.csv"
```
------------------------------------------------------------------------------------
Then, we now should send messages to llm: 
something like:
```
[
    "you are a data scientist, and your role is help generate code to analyze data users provided",
    "generate (Python) code to analyze data.csv",
]
- Should tweak/prompt engineer for better result.
- Make sure to grab file path from request.reference ----> request.references[0].value gives absolute path to file mentioned with #file
```
------------------------------------------------------------------------------------
Then, parse response, get code

```
import pandas as pd
df = pd.create_csv('data.csv')
...
```
------------------------------------------------------------------------------------
Then, call jupyter execution api 
grab execution result response returned from jupyter kernel 

------------------------------------------------------------------------------------
Then, send the execution result response to LLM
```
[
  system: "you are data agent ... + other prompt engineering"
  user: 'i want to analyze data.csv"
  "here is the code to gather some info /.."
  "here is the execution result of the code"
  "please do the analysis based on above info" 
]
```
------------------------------------------------------------------------------------
------------------------------------------------------------------------------------
------------------------------------------------------------------------------------

Idea on letting the LLM talk to Jupyter Kernel on behalf of us:
- This would happen via function call and basically providing a wrapper/function call around providing access to jupyter kernel to allow LLM to execute & get the execution result.

----> This way we provide the function/engine/kernel that LLM can directly talk to and get code execution result, rather than us having to be a middle man.
-- Middle Man scenario: Us (developers) take the code generated by LLM and go get the response from Jupyter kernel, then we provide the LLM with these response,
and then ask again for analysis given the execution result. 

```
[
    "you are a data scientist, and your role is help generate code to analyze data users provided",
    "users want to do 'analyze data.csv'",

    here are all the functions available to you (include function/wrapper)
    {
       "run_code": {
          "parameters: ''
          "return": string
       }
    }
]
```
